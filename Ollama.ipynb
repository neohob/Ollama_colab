{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzhPCvtlIjClh18lsSxPKX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohob/Ollama_colab/blob/main/Ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Get ollama and run ollama but don't stop it**"
      ],
      "metadata": {
        "id": "-ebDukDJ4l04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0V3EZ5R2rgz",
        "outputId": "edc6bbf3-364f-473b-d310-17d3b837646c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Install Ngrok python library**"
      ],
      "metadata": {
        "id": "me-aThXY4zBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVZe7y9z4_Pi",
        "outputId": "8a59e52f-b324-44e8-e7e2-1e8fcb58b474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.2-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Set up ngrok and forward the local ollama service to a public URI**\n",
        "\n",
        "*Modify token to your own ngrok token, you can go to ngrok.com to register and get token*\n",
        "\n",
        "Ollama isn't yet running as a service but we can set up ngrok in advance of this"
      ],
      "metadata": {
        "id": "fBsfhhnw5BJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "import os\n",
        "import asyncio\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "from threading import Thread\n",
        "\n",
        "# Get your ngrok token from your ngrok account:\n",
        "# https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "token=\"Your Ngrok token\"\n",
        "ngrok.set_auth_token(token)\n",
        "\n",
        "# set up a stoppable thread (not mandatory, but cleaner if you want to stop this later\n",
        "class StoppableThread(threading.Thread):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(StoppableThread, self).__init__(*args, **kwargs)\n",
        "        self._stop_event = threading.Event()\n",
        "\n",
        "    def stop(self):\n",
        "        self._stop_event.set()\n",
        "\n",
        "    def is_stopped(self):\n",
        "        return self._stop_event.is_set()\n",
        "\n",
        "def start_ngrok(q, stop_event):\n",
        "    try:\n",
        "        # Start an HTTP tunnel on the specified port\n",
        "        public_url = ngrok.connect(11434)\n",
        "        # Put the public URL in the queue\n",
        "        q.put(public_url)\n",
        "        # Keep the thread alive until stop event is set\n",
        "        while not stop_event.is_set():\n",
        "            time.sleep(1)  # Adjust sleep time as needed\n",
        "    except Exception as e:\n",
        "        print(f\"Error in start_ngrok: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzCtm02b3_RB",
        "outputId": "7ee3a833-f3d7-4ffb-af6a-2bc3e47cd3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run that code so the functions exist, then in the next cell, start ngrok in a separate thread so it doesn't hang your colab - we'll use a queue so we can still share data between threads because we want to know what the ngrok public URL will be when it runs"
      ],
      "metadata": {
        "id": "PgMf0z_K7S91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a queue to share data between threads\n",
        "url_queue = queue.Queue()\n",
        "\n",
        "# Start ngrok in a separate thread\n",
        "ngrok_thread = StoppableThread(target=start_ngrok, args=(url_queue, StoppableThread.is_stopped))\n",
        "ngrok_thread.start()"
      ],
      "metadata": {
        "id": "hr61QOZ85WiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That will be running, but you need to get the results from the queue to see what ngrok returned, so then do:"
      ],
      "metadata": {
        "id": "igEwFFYh7bqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wait for the ngrok tunnel to be established\n",
        "while True:\n",
        "    try:\n",
        "        public_url = url_queue.get()\n",
        "        if public_url:\n",
        "            break\n",
        "        print(\"Waiting for ngrok URL...\")\n",
        "        time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in retrieving ngrok URL: {e}\")\n",
        "\n",
        "print(\"Ngrok tunnel established at:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo8igJ5m5cmA",
        "outputId": "a5f7b0c4-7612-4cc3-9529-c4ace46e458d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok tunnel established at: NgrokTunnel: \"https://7f2d-34-173-97-195.ngrok-free.app\" -> \"http://localhost:11434\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This should output something like:\n",
        "\n",
        "*Ngrok tunnel established at: NgrokTunnel: \"https://{somelongsubdomain}.ngrok-free.app\" -> \"http://localhost:11434\"*"
      ],
      "metadata": {
        "id": "xAK5phcI7nWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Run ollama as an async process**"
      ],
      "metadata": {
        "id": "SPrSyIFv7tSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "\n",
        "# NB: You may need to set these depending and get cuda working depending which backend you are running.\n",
        "# Set environment variable for NVIDIA library\n",
        "# Set environment variables for CUDA\n",
        "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "# Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories\n",
        "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
        "\n",
        "async def run_process(cmd):\n",
        "    print('>>> starting', *cmd)\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        *cmd,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # define an async pipe function\n",
        "    async def pipe(lines):\n",
        "        async for line in lines:\n",
        "            print(line.decode().strip())\n",
        "\n",
        "        await asyncio.gather(\n",
        "            pipe(process.stdout),\n",
        "            pipe(process.stderr),\n",
        "        )\n",
        "\n",
        "    # call it\n",
        "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))"
      ],
      "metadata": {
        "id": "2wcgYs5T5g-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That creates the function to run an async command but doesn't run it yet.\n",
        "\n",
        "This will start ollama in a separate thread so your Colab isn't blocked:"
      ],
      "metadata": {
        "id": "vAu8rmeO70MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import threading\n",
        "\n",
        "async def start_ollama_serve():\n",
        "    await run_process(['ollama', 'serve'])\n",
        "\n",
        "def run_async_in_thread(loop, coro):\n",
        "    asyncio.set_event_loop(loop)\n",
        "    loop.run_until_complete(coro)\n",
        "    loop.close()\n",
        "\n",
        "# Create a new event loop that will run in a new thread\n",
        "new_loop = asyncio.new_event_loop()\n",
        "\n",
        "# Start ollama serve in a separate thread so the cell won't block execution\n",
        "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
        "thread.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JRjylGl5mSH",
        "outputId": "3f1219bb-a05d-46c3-f68a-8ce391ffbc25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> starting ollama serve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should produce something like:\n",
        "\n",
        ">>> starting ollama serve\n",
        "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
        "Your new public key is:\n",
        "\n",
        "ssh-ed25519 {some key}\n",
        "\n",
        "2024/01/16 20:19:11 images.go:808: total blobs: 0\n",
        "2024/01/16 20:19:11 images.go:815: total unused blobs removed: 0\n",
        "2024/01/16 20:19:11 routes.go:930: Listening on 127.0.0.1:11434 (version 0.1.26)\n",
        "\n"
      ],
      "metadata": {
        "id": "0ik09OWu8AhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now you're all set up. You can either do the next steps in the Colab, but it might be easier to run on your local machine if you normally dev there.**"
      ],
      "metadata": {
        "id": "GSKYSgx_8Jpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Run an ollama model remotely from your local dev environment**\n",
        "\n",
        "Assuming you have installed ollama on your local dev environment (say WSL2), I'm assuming it's linux anyway... but i.e. your laptop or desktop machine in front of you (as opposed to Colab).\n",
        "\n",
        "Replace the actual URI below with whatever public URI ngrok reported above:\n",
        "\n",
        "*export OLLAMA_HOST=https://{longcode}.ngrok-free.app/*\n",
        "\n",
        "You can now run ollama and it will run on the remote in your Colab (so long as that's stays up and running).\n",
        "\n",
        "e.g. run this on your local machine and it will look as if it's running locally but it's really running in your Colab and the results are being served to wherever you call this from (so long as the OLLAMA_HOST is set correctly and is a valid tunnel to your ollama service:\n",
        "\n",
        "*ollama run mistral*\n",
        "\n",
        "You can now interact with the model on the command line locally but the model runs on the Colab.\n",
        "\n",
        "If you want to run larger models, like mixtral, then you need to be sure to connect your Colab to a Back end compute that's powerful enough (e.g. 48GB+ of RAM, so V100 GPU is minimum spec for this at the time of writing).\n",
        "\n",
        "Note: If you have any issues with cuda or nvidia showing in the ouputs of any steps above, don't proceed until you fix them."
      ],
      "metadata": {
        "id": "miHbP6718QiE"
      }
    }
  ]
}